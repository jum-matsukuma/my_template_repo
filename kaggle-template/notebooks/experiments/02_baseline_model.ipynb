{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model Development\n",
    "\n",
    "**Competition**: [Competition Name]  \n",
    "**Author**: [Your Name]  \n",
    "**Date**: [Date]  \n",
    "**Objective**: Create competitive baseline models and establish validation framework\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Data Loading](#1-setup-and-data-loading)\n",
    "2. [Feature Preprocessing](#2-feature-preprocessing)\n",
    "3. [Cross-Validation Setup](#3-cross-validation-setup)\n",
    "4. [Simple Baseline](#4-simple-baseline)\n",
    "5. [LightGBM Baseline](#5-lightgbm-baseline)\n",
    "6. [Model Comparison](#6-model-comparison)\n",
    "7. [Submission Generation](#7-submission-generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('../../data/raw/train.csv')\n",
    "test_df = pd.read_csv('../../data/raw/test.csv')\n",
    "sample_submission = pd.read_csv('../../data/raw/sample_submission.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "# Load EDA insights\n",
    "import json\n",
    "with open('../../configs/eda_insights.json', 'r') as f:\n",
    "    eda_insights = json.load(f)\n",
    "\n",
    "target_col = eda_insights['target_info']['name']\n",
    "print(f\"Target column: {target_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define competition metric function\n",
    "def competition_metric(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Define the competition evaluation metric here\n",
    "    Update this based on the specific competition\n",
    "    \"\"\"\n",
    "    # Example: RMSE for regression\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    # Example: Log loss for classification\n",
    "    # from sklearn.metrics import log_loss\n",
    "    # return log_loss(y_true, y_pred)\n",
    "\n",
    "# Test the metric function\n",
    "dummy_true = np.array([1, 2, 3, 4, 5])\n",
    "dummy_pred = np.array([1.1, 2.2, 2.9, 4.1, 4.9])\n",
    "print(f\"Metric test: {competition_metric(dummy_true, dummy_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature types\n",
    "numerical_features = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove target and ID columns\n",
    "if target_col in numerical_features:\n",
    "    numerical_features.remove(target_col)\n",
    "\n",
    "# Remove ID column if exists (update based on competition)\n",
    "id_cols = ['id', 'Id', 'ID']  # Common ID column names\n",
    "for col in id_cols:\n",
    "    if col in numerical_features:\n",
    "        numerical_features.remove(col)\n",
    "    if col in categorical_features:\n",
    "        categorical_features.remove(col)\n",
    "\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "print(f\"Total features: {len(numerical_features) + len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(train_df, test_df, numerical_features, categorical_features):\n",
    "    \"\"\"\n",
    "    Basic feature preprocessing pipeline\n",
    "    \"\"\"\n",
    "    # Copy dataframes\n",
    "    train_processed = train_df.copy()\n",
    "    test_processed = test_df.copy()\n",
    "    \n",
    "    # Handle missing values for numerical features\n",
    "    for col in numerical_features:\n",
    "        if train_processed[col].isnull().sum() > 0:\n",
    "            # Fill with median\n",
    "            median_val = train_processed[col].median()\n",
    "            train_processed[col].fillna(median_val, inplace=True)\n",
    "            test_processed[col].fillna(median_val, inplace=True)\n",
    "    \n",
    "    # Handle missing values and encode categorical features\n",
    "    label_encoders = {}\n",
    "    for col in categorical_features:\n",
    "        # Fill missing values\n",
    "        train_processed[col].fillna('Unknown', inplace=True)\n",
    "        test_processed[col].fillna('Unknown', inplace=True)\n",
    "        \n",
    "        # Label encode\n",
    "        le = LabelEncoder()\n",
    "        \n",
    "        # Fit on combined data to handle unseen categories in test\n",
    "        combined_values = pd.concat([train_processed[col], test_processed[col]]).unique()\n",
    "        le.fit(combined_values)\n",
    "        \n",
    "        train_processed[col] = le.transform(train_processed[col])\n",
    "        test_processed[col] = le.transform(test_processed[col])\n",
    "        \n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    return train_processed, test_processed, label_encoders\n",
    "\n",
    "# Apply preprocessing\n",
    "train_processed, test_processed, label_encoders = preprocess_features(\n",
    "    train_df, test_df, numerical_features, categorical_features\n",
    ")\n",
    "\n",
    "print(\"Preprocessing completed!\")\n",
    "print(f\"Training data shape: {train_processed.shape}\")\n",
    "print(f\"Test data shape: {test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross-Validation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "feature_cols = numerical_features + categorical_features\n",
    "X = train_processed[feature_cols]\n",
    "y = train_processed[target_col]\n",
    "X_test = test_processed[feature_cols]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation strategy\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Choose CV strategy based on target type\n",
    "if eda_insights['target_info']['type'] == 'categorical':\n",
    "    cv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    print(\"Using StratifiedKFold for classification\")\nelse:\n",
    "    cv = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    print(\"Using KFold for regression\")\n",
    "\n",
    "def cross_validate_model(model, X, y, cv, scoring_func):\n",
    "    \"\"\"\n",
    "    Perform cross-validation and return OOF predictions and scores\n",
    "    \"\"\"\n",
    "    oof_predictions = np.zeros(len(X))\n",
    "    cv_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "        # Split data\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        val_pred = model.predict(X_val)\n",
    "        oof_predictions[val_idx] = val_pred\n",
    "        \n",
    "        # Calculate fold score\n",
    "        fold_score = scoring_func(y_val, val_pred)\n",
    "        cv_scores.append(fold_score)\n",
    "        \n",
    "        print(f\"Fold {fold + 1}: {fold_score:.6f}\")\n",
    "    \n",
    "    # Overall score\n",
    "    overall_score = scoring_func(y, oof_predictions)\n",
    "    \n",
    "    print(f\"Mean CV: {np.mean(cv_scores):.6f} (+/- {np.std(cv_scores):.6f})\")\n",
    "    print(f\"Overall OOF: {overall_score:.6f}\")\n",
    "    \n",
    "    return {\n",
    "        'oof_predictions': oof_predictions,\n",
    "        'cv_scores': cv_scores,\n",
    "        'mean_cv': np.mean(cv_scores),\n",
    "        'std_cv': np.std(cv_scores),\n",
    "        'overall_score': overall_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple baseline: predict mean (for regression) or mode (for classification)\n",
    "print(\"=== SIMPLE BASELINE ===\")\n",
    "\n",
    "if eda_insights['target_info']['type'] == 'categorical':\n",
    "    # Mode prediction for classification\n",
    "    baseline_pred = y.mode()[0]\n",
    "    baseline_predictions = np.full(len(y), baseline_pred)\n",
    "else:\n",
    "    # Mean prediction for regression\n",
    "    baseline_pred = y.mean()\n",
    "    baseline_predictions = np.full(len(y), baseline_pred)\n",
    "\n",
    "baseline_score = competition_metric(y, baseline_predictions)\n",
    "print(f\"Simple baseline score: {baseline_score:.6f}\")\n",
    "print(f\"Baseline prediction value: {baseline_pred:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest baseline\n",
    "print(\"\\n=== RANDOM FOREST BASELINE ===\")\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_results = cross_validate_model(rf_model, X, y, cv, competition_metric)\n",
    "rf_cv_score = rf_results['mean_cv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LightGBM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM baseline\n",
    "print(\"\\n=== LIGHTGBM BASELINE ===\")\n",
    "\n",
    "# LightGBM parameters\n",
    "lgb_params = {\n",
    "    'objective': 'regression',  # Update based on competition\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 20,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "def lgb_cross_validate(X, y, cv, params, num_boost_round=1000):\n",
    "    \"\"\"\n",
    "    LightGBM cross-validation with early stopping\n",
    "    \"\"\"\n",
    "    oof_predictions = np.zeros(len(X))\n",
    "    cv_scores = []\n",
    "    feature_importance = np.zeros(X.shape[1])\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "        # Split data\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Create LightGBM datasets\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "        \n",
    "        # Train model\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            num_boost_round=num_boost_round,\n",
    "            valid_sets=[train_data, val_data],\n",
    "            valid_names=['train', 'val'],\n",
    "            callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        \n",
    "        # Predict\n",
    "        val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "        oof_predictions[val_idx] = val_pred\n",
    "        \n",
    "        # Calculate fold score\n",
    "        fold_score = competition_metric(y_val, val_pred)\n",
    "        cv_scores.append(fold_score)\n",
    "        \n",
    "        # Accumulate feature importance\n",
    "        feature_importance += model.feature_importance(importance_type='gain')\n",
    "        \n",
    "        print(f\"Fold {fold + 1}: {fold_score:.6f} (best_iteration: {model.best_iteration})\")\n",
    "    \n",
    "    # Average feature importance\n",
    "    feature_importance /= N_FOLDS\n",
    "    \n",
    "    # Overall score\n",
    "    overall_score = competition_metric(y, oof_predictions)\n",
    "    \n",
    "    print(f\"Mean CV: {np.mean(cv_scores):.6f} (+/- {np.std(cv_scores):.6f})\")\n",
    "    print(f\"Overall OOF: {overall_score:.6f}\")\n",
    "    \n",
    "    return {\n",
    "        'oof_predictions': oof_predictions,\n",
    "        'cv_scores': cv_scores,\n",
    "        'mean_cv': np.mean(cv_scores),\n",
    "        'std_cv': np.std(cv_scores),\n",
    "        'overall_score': overall_score,\n",
    "        'feature_importance': feature_importance\n",
    "    }\n",
    "\n",
    "# Train LightGBM\n",
    "lgb_results = lgb_cross_validate(X, y, cv, lgb_params)\n",
    "lgb_cv_score = lgb_results['mean_cv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': lgb_results['feature_importance']\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = feature_importance_df.head(20)\n",
    "sns.barplot(data=top_features, x='importance', y='feature')\n",
    "plt.title('Top 20 Feature Importance (LightGBM)')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 most important features:\")\n",
    "print(feature_importance_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline models\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Model': ['Simple Baseline', 'Random Forest', 'LightGBM'],\n",
    "    'CV Score': [baseline_score, rf_cv_score, lgb_cv_score],\n",
    "    'CV Std': [0, rf_results['std_cv'], lgb_results['std_cv']]\n",
    "})\n",
    "\n",
    "model_comparison = model_comparison.sort_values('CV Score')\n",
    "print(\"Model Comparison:\")\n",
    "display(model_comparison)\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(model_comparison['Model'], model_comparison['CV Score'])\n",
    "plt.errorbar(model_comparison['Model'], model_comparison['CV Score'], \n",
    "             yerr=model_comparison['CV Std'], fmt='none', capsize=5, color='black')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Competition Metric Score')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, score) in enumerate(zip(bars, model_comparison['CV Score'])):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + model_comparison['CV Std'].iloc[i]/2, \n",
    "             f'{score:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test set using best model (LightGBM)\n",
    "print(\"Generating test predictions with LightGBM...\")\n",
    "\n",
    "def train_full_lgb_model(X, y, X_test, params, num_boost_round=1000):\n",
    "    \"\"\"\n",
    "    Train LightGBM on full training data and predict on test\n",
    "    \"\"\"\n",
    "    # Create dataset\n",
    "    train_data = lgb.Dataset(X, label=y)\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=num_boost_round,\n",
    "        callbacks=[lgb.log_evaluation(100)]\n",
    "    )\n",
    "    \n",
    "    # Predict on test\n",
    "    test_predictions = model.predict(X_test)\n",
    "    \n",
    "    return test_predictions, model\n",
    "\n",
    "# Train and predict\n",
    "test_predictions, final_model = train_full_lgb_model(X, y, X_test, lgb_params)\n",
    "\n",
    "print(f\"Test predictions shape: {test_predictions.shape}\")\n",
    "print(f\"Test predictions range: {test_predictions.min():.4f} to {test_predictions.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = sample_submission.copy()\n",
    "\n",
    "# Update target column (check sample_submission format)\n",
    "target_col_submission = submission.columns[-1]  # Usually the last column\n",
    "submission[target_col_submission] = test_predictions\n",
    "\n",
    "# Save submission\n",
    "submission_filename = '../../submissions/lgb_baseline_submission.csv'\n",
    "submission.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"Submission saved to: {submission_filename}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "display(submission.head())\n",
    "display(submission.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline results for future reference\n",
    "baseline_results = {\n",
    "    'model_comparison': model_comparison.to_dict('records'),\n",
    "    'best_model': 'LightGBM',\n",
    "    'best_cv_score': float(lgb_cv_score),\n",
    "    'lgb_params': lgb_params,\n",
    "    'feature_importance': feature_importance_df.head(20).to_dict('records'),\n",
    "    'preprocessing': {\n",
    "        'numerical_features': numerical_features,\n",
    "        'categorical_features': categorical_features,\n",
    "        'total_features': len(feature_cols)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results\n",
    "with open('../../configs/baseline_results.json', 'w') as f:\n",
    "    json.dump(baseline_results, f, indent=2)\n",
    "\n",
    "print(\"Baseline results saved to configs/baseline_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings:\n",
    "- **Best Baseline Model**: LightGBM\n",
    "- **CV Score**: {lgb_cv_score:.6f} (+/- {lgb_results['std_cv']:.6f})\n",
    "- **Key Features**: Top features identified through importance analysis\n",
    "- **Next Steps**: \n",
    "  - Feature engineering based on importance analysis\n",
    "  - Hyperparameter optimization\n",
    "  - Advanced preprocessing techniques\n",
    "  - Ensemble methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.11.0"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}