{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Submission Generation\n",
    "\n",
    "**Competition**: [Competition Name]  \n",
    "**Author**: [Your Name]  \n",
    "**Date**: [Date]  \n",
    "**Objective**: Generate final ensemble predictions for competition submission\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Model Loading](#1-setup-and-model-loading)\n",
    "2. [Ensemble Strategy](#2-ensemble-strategy)\n",
    "3. [Final Model Training](#3-final-model-training)\n",
    "4. [Validation and Sanity Checks](#4-validation-and-sanity-checks)\n",
    "5. [Submission Generation](#5-submission-generation)\n",
    "6. [Documentation](#6-documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and previous results\n",
    "train_df = pd.read_csv('../../data/processed/train_processed.csv')  # Use processed data\n",
    "test_df = pd.read_csv('../../data/processed/test_processed.csv')\n",
    "sample_submission = pd.read_csv('../../data/raw/sample_submission.csv')\n",
    "\n",
    "# Load configuration from previous experiments\n",
    "with open('../../configs/baseline_results.json', 'r') as f:\n",
    "    baseline_config = json.load(f)\n",
    "\n",
    "with open('../../configs/eda_insights.json', 'r') as f:\n",
    "    eda_insights = json.load(f)\n",
    "\n",
    "target_col = eda_insights['target_info']['name']\n",
    "feature_cols = baseline_config['preprocessing']['numerical_features'] + baseline_config['preprocessing']['categorical_features']\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Target column: {target_col}\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final feature set\n",
    "X = train_df[feature_cols]\n",
    "y = train_df[target_col]\n",
    "X_test = test_df[feature_cols]\n",
    "\n",
    "print(f\"Training features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "\n",
    "# Competition metric function\n",
    "def competition_metric(y_true, y_pred):\n",
    "    \"\"\"Competition evaluation metric\"\"\"\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))  # Update based on competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ensemble Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation setup\n",
    "N_FOLDS = 5\n",
    "if eda_insights['target_info']['type'] == 'categorical':\n",
    "    cv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\nelse:\n",
    "    cv = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "print(f\"Using {'StratifiedKFold' if eda_insights['target_info']['type'] == 'categorical' else 'KFold'} with {N_FOLDS} folds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations for ensemble\n",
    "models_config = {\n",
    "    'lgb': {\n",
    "        'params': {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'min_child_samples': 20,\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'verbose': -1\n",
    "        },\n",
    "        'num_rounds': 1000\n",
    "    },\n",
    "    'xgb': {\n",
    "        'params': {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'eval_metric': 'rmse',\n",
    "            'eta': 0.05,\n",
    "            'max_depth': 6,\n",
    "            'min_child_weight': 1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'verbosity': 0\n",
    "        },\n",
    "        'num_rounds': 1000\n",
    "    },\n",
    "    'cb': {\n",
    "        'params': {\n",
    "            'objective': 'RMSE',\n",
    "            'learning_rate': 0.05,\n",
    "            'depth': 6,\n",
    "            'l2_leaf_reg': 3,\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'verbose': False\n",
    "        },\n",
    "        'num_rounds': 1000\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Ensemble will use {len(models_config)} models: {list(models_config.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble_models(X, y, X_test, models_config, cv):\n",
    "    \"\"\"\n",
    "    Train ensemble of models with cross-validation\n",
    "    \"\"\"\n",
    "    oof_predictions = {}\n",
    "    test_predictions = {}\n",
    "    cv_scores = {}\n",
    "    \n",
    "    for model_name, config in models_config.items():\n",
    "        print(f\"\\n=== Training {model_name.upper()} ===\")\n",
    "        \n",
    "        oof_pred = np.zeros(len(X))\n",
    "        test_pred = np.zeros(len(X_test))\n",
    "        fold_scores = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            if model_name == 'lgb':\n",
    "                train_data = lgb.Dataset(X_train, label=y_train)\n",
    "                val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "                \n",
    "                model = lgb.train(\n",
    "                    config['params'],\n",
    "                    train_data,\n",
    "                    num_boost_round=config['num_rounds'],\n",
    "                    valid_sets=[val_data],\n",
    "                    callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]\n",
    "                )\n",
    "                \n",
    "                val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "                test_fold_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "                \n",
    "            elif model_name == 'xgb':\n",
    "                dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "                dval = xgb.DMatrix(X_val, label=y_val)\n",
    "                dtest = xgb.DMatrix(X_test)\n",
    "                \n",
    "                model = xgb.train(\n",
    "                    config['params'],\n",
    "                    dtrain,\n",
    "                    num_boost_round=config['num_rounds'],\n",
    "                    evals=[(dval, 'val')],\n",
    "                    early_stopping_rounds=100,\n",
    "                    verbose_eval=0\n",
    "                )\n",
    "                \n",
    "                val_pred = model.predict(dval)\n",
    "                test_fold_pred = model.predict(dtest)\n",
    "                \n",
    "            elif model_name == 'cb':\n",
    "                model = cb.CatBoostRegressor(\n",
    "                    iterations=config['num_rounds'],\n",
    "                    **config['params']\n",
    "                )\n",
    "                \n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=(X_val, y_val),\n",
    "                    early_stopping_rounds=100,\n",
    "                    verbose=False\n",
    "                )\n",
    "                \n",
    "                val_pred = model.predict(X_val)\n",
    "                test_fold_pred = model.predict(X_test)\n",
    "            \n",
    "            # Store predictions\n",
    "            oof_pred[val_idx] = val_pred\n",
    "            test_pred += test_fold_pred / N_FOLDS\n",
    "            \n",
    "            # Calculate fold score\n",
    "            fold_score = competition_metric(y_val, val_pred)\n",
    "            fold_scores.append(fold_score)\n",
    "            \n",
    "            print(f\"Fold {fold + 1}: {fold_score:.6f}\")\n",
    "        \n",
    "        # Store results\n",
    "        oof_predictions[model_name] = oof_pred\n",
    "        test_predictions[model_name] = test_pred\n",
    "        cv_scores[model_name] = {\n",
    "            'mean': np.mean(fold_scores),\n",
    "            'std': np.std(fold_scores),\n",
    "            'scores': fold_scores\n",
    "        }\n",
    "        \n",
    "        overall_score = competition_metric(y, oof_pred)\n",
    "        print(f\"Mean CV: {np.mean(fold_scores):.6f} (+/- {np.std(fold_scores):.6f})\")\n",
    "        print(f\"Overall OOF: {overall_score:.6f}\")\n",
    "    \n",
    "    return oof_predictions, test_predictions, cv_scores\n",
    "\n",
    "# Train all models\n",
    "oof_preds, test_preds, cv_scores = train_ensemble_models(X, y, X_test, models_config, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance comparison\n",
    "model_performance = pd.DataFrame({\n",
    "    'Model': list(cv_scores.keys()),\n",
    "    'CV_Mean': [cv_scores[model]['mean'] for model in cv_scores.keys()],\n",
    "    'CV_Std': [cv_scores[model]['std'] for model in cv_scores.keys()]\n",
    "})\n",
    "\n",
    "model_performance = model_performance.sort_values('CV_Mean')\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "display(model_performance)\n",
    "\n",
    "# Visualize performance\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(model_performance['Model'], model_performance['CV_Mean'])\n",
    "plt.errorbar(model_performance['Model'], model_performance['CV_Mean'], \n",
    "             yerr=model_performance['CV_Std'], fmt='none', capsize=5, color='black')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('CV Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble combination strategies\n",
    "def optimize_ensemble_weights(oof_predictions, y_true, metric_func):\n",
    "    \"\"\"\n",
    "    Optimize ensemble weights using simple average and Ridge regression\n",
    "    \"\"\"\n",
    "    # Prepare OOF predictions matrix\n",
    "    oof_matrix = np.column_stack([oof_predictions[model] for model in oof_predictions.keys()])\n",
    "    \n",
    "    # Simple average\n",
    "    simple_avg = np.mean(oof_matrix, axis=1)\n",
    "    simple_score = metric_func(y_true, simple_avg)\n",
    "    \n",
    "    # Ridge regression for weight optimization\n",
    "    ridge = Ridge(alpha=1.0, fit_intercept=False, positive=True)\n",
    "    ridge.fit(oof_matrix, y_true)\n",
    "    ridge_weights = ridge.coef_ / np.sum(ridge.coef_)  # Normalize weights\n",
    "    ridge_pred = np.dot(oof_matrix, ridge_weights)\n",
    "    ridge_score = metric_func(y_true, ridge_pred)\n",
    "    \n",
    "    return {\n",
    "        'simple_avg': {'score': simple_score, 'weights': np.ones(len(oof_predictions)) / len(oof_predictions)},\n",
    "        'ridge': {'score': ridge_score, 'weights': ridge_weights}\n",
    "    }\n",
    "\n",
    "# Optimize ensemble\n",
    "ensemble_results = optimize_ensemble_weights(oof_preds, y, competition_metric)\n",
    "\n",
    "print(\"Ensemble Performance:\")\n",
    "for method, result in ensemble_results.items():\n",
    "    print(f\"{method}: {result['score']:.6f}\")\n",
    "    print(f\"  Weights: {dict(zip(oof_preds.keys(), result['weights']))}\")\n",
    "\n",
    "# Choose best ensemble method\n",
    "best_ensemble = min(ensemble_results.items(), key=lambda x: x[1]['score'])\n",
    "best_method, best_result = best_ensemble\n",
    "print(f\"\\nBest ensemble method: {best_method} (score: {best_result['score']:.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validation and Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final test predictions using best ensemble\n",
    "final_weights = best_result['weights']\n",
    "final_test_pred = np.zeros(len(X_test))\n",
    "\n",
    "for i, (model_name, weight) in enumerate(zip(test_preds.keys(), final_weights)):\n",
    "    final_test_pred += weight * test_preds[model_name]\n",
    "    print(f\"{model_name}: weight = {weight:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal test predictions shape: {final_test_pred.shape}\")\n",
    "print(f\"Prediction range: {final_test_pred.min():.4f} to {final_test_pred.max():.4f}\")\n",
    "print(f\"Prediction mean: {final_test_pred.mean():.4f}\")\n",
    "print(f\"Prediction std: {final_test_pred.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Prediction distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(final_test_pred, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Test Predictions Distribution')\n",
    "plt.xlabel('Predicted Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Compare with training target distribution\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(y, bins=50, alpha=0.7, label='Train Target', edgecolor='black')\n",
    "plt.hist(final_test_pred, bins=50, alpha=0.7, label='Test Predictions', edgecolor='black')\n",
    "plt.title('Train vs Test Distribution')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Model correlation matrix\n",
    "plt.subplot(1, 3, 3)\n",
    "pred_df = pd.DataFrame(oof_preds)\n",
    "correlation_matrix = pred_df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, square=True)\n",
    "plt.title('Model Prediction Correlations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print correlation matrix\n",
    "print(\"\\nModel Prediction Correlations:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final submission\n",
    "submission = sample_submission.copy()\n",
    "target_col_submission = submission.columns[-1]  # Usually the last column\n",
    "submission[target_col_submission] = final_test_pred\n",
    "\n",
    "# Generate unique filename\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "submission_filename = f'../../submissions/final_ensemble_submission_{timestamp}.csv'\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"Final submission saved to: {submission_filename}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Expected CV score: {best_result['score']:.6f}\")\n",
    "\n",
    "display(submission.head())\n",
    "display(submission.tail())\n",
    "print(f\"\\nSubmission statistics:\")\n",
    "print(submission[target_col_submission].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document final submission details\n",
    "submission_log = {\n",
    "    'timestamp': timestamp,\n",
    "    'filename': submission_filename,\n",
    "    'expected_cv_score': float(best_result['score']),\n",
    "    'ensemble_method': best_method,\n",
    "    'model_weights': {model: float(weight) for model, weight in zip(oof_preds.keys(), final_weights)},\n",
    "    'individual_model_scores': {model: float(cv_scores[model]['mean']) for model in cv_scores.keys()},\n",
    "    'model_configs': models_config,\n",
    "    'features_used': feature_cols,\n",
    "    'total_features': len(feature_cols),\n",
    "    'cv_strategy': f\"{'StratifiedKFold' if eda_insights['target_info']['type'] == 'categorical' else 'KFold'} (n_splits={N_FOLDS})\",\n",
    "    'preprocessing': 'Standard preprocessing with median imputation and label encoding',\n",
    "    'prediction_stats': {\n",
    "        'min': float(final_test_pred.min()),\n",
    "        'max': float(final_test_pred.max()),\n",
    "        'mean': float(final_test_pred.mean()),\n",
    "        'std': float(final_test_pred.std())\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save submission log\n",
    "log_filename = f'../../experiments/submission_log_{timestamp}.json'\n",
    "with open(log_filename, 'w') as f:\n",
    "    json.dump(submission_log, f, indent=2)\n",
    "\n",
    "print(f\"Submission log saved to: {log_filename}\")\n",
    "\n",
    "# Create summary for SKILLS update\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL SUBMISSION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Expected CV Score: {best_result['score']:.6f}\")\n",
    "print(f\"Ensemble Method: {best_method}\")\n",
    "print(f\"Models Used: {list(oof_preds.keys())}\")\n",
    "print(f\"Best Individual Model: {model_performance.iloc[0]['Model']} ({model_performance.iloc[0]['CV_Mean']:.6f})\")\n",
    "print(f\"Ensemble Improvement: {model_performance.iloc[0]['CV_Mean'] - best_result['score']:.6f}\")\n",
    "print(f\"Total Features: {len(feature_cols)}\")\n",
    "print(f\"Submission File: {submission_filename}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional backup submission (simple average)\n",
    "simple_avg_pred = np.mean([test_preds[model] for model in test_preds.keys()], axis=0)\n",
    "backup_submission = sample_submission.copy()\n",
    "backup_submission[target_col_submission] = simple_avg_pred\n",
    "\n",
    "backup_filename = f'../../submissions/backup_simple_avg_{timestamp}.csv'\n",
    "backup_submission.to_csv(backup_filename, index=False)\n",
    "\n",
    "print(f\"\\nBackup submission (simple average) saved to: {backup_filename}\")\n",
    "print(f\"Backup expected score: {ensemble_results['simple_avg']['score']:.6f}\")"
   ]
  }
 ],\n "metadata": {\n  "kernelspec": {\n   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.11.0"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}